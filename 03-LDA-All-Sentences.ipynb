{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "\n",
    "import gensim\n",
    "import pyLDAvis\n",
    "from pyLDAvis import gensim as gensimvis\n",
    "import spacy\n",
    "\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORT DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('pizza_sentences.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_index</th>\n",
       "      <th>sentence_index</th>\n",
       "      <th>sentence</th>\n",
       "      <th>review_id</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>I'll be the first to admit that I was not exci...</td>\n",
       "      <td>fdiNeiN_hoCxCMy2wTRW9g</td>\n",
       "      <td>[admit, tavolta]</td>\n",
       "      <td>(1, Negative)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Being a food snob, when a group of friends su...</td>\n",
       "      <td>fdiNeiN_hoCxCMy2wTRW9g</td>\n",
       "      <td>[food, snob, group, friend, dinner, menu]</td>\n",
       "      <td>(1, Negative)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Im also not big on ordering pasta when I go out</td>\n",
       "      <td>fdiNeiN_hoCxCMy2wTRW9g</td>\n",
       "      <td>[ordering, pasta]</td>\n",
       "      <td>(1, Negative)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Alas, I was outnumbered</td>\n",
       "      <td>fdiNeiN_hoCxCMy2wTRW9g</td>\n",
       "      <td>[]</td>\n",
       "      <td>(2, Neutral)</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>Thank goodness! I ordered the sea bass special</td>\n",
       "      <td>fdiNeiN_hoCxCMy2wTRW9g</td>\n",
       "      <td>[goodness, sea, bass]</td>\n",
       "      <td>(3, Positive)</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_index  sentence_index  \\\n",
       "0             5               0   \n",
       "1             5               1   \n",
       "2             5               2   \n",
       "3             5               3   \n",
       "4             5               4   \n",
       "\n",
       "                                            sentence               review_id  \\\n",
       "0  I'll be the first to admit that I was not exci...  fdiNeiN_hoCxCMy2wTRW9g   \n",
       "1   Being a food snob, when a group of friends su...  fdiNeiN_hoCxCMy2wTRW9g   \n",
       "2    Im also not big on ordering pasta when I go out  fdiNeiN_hoCxCMy2wTRW9g   \n",
       "3                            Alas, I was outnumbered  fdiNeiN_hoCxCMy2wTRW9g   \n",
       "4     Thank goodness! I ordered the sea bass special  fdiNeiN_hoCxCMy2wTRW9g   \n",
       "\n",
       "                                     cleaned      sentiment  sentiment_val  \n",
       "0                           [admit, tavolta]  (1, Negative)            1.0  \n",
       "1  [food, snob, group, friend, dinner, menu]  (1, Negative)            1.0  \n",
       "2                          [ordering, pasta]  (1, Negative)            1.0  \n",
       "3                                         []   (2, Neutral)            2.0  \n",
       "4                      [goodness, sea, bass]  (3, Positive)            3.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df['cleaned'] = [w_tokenizer.tokenize(X.strip('][').replace(',','')) for X in df['cleaned']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(728570, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SENTIMENT ANALYSIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: '-LRB- I 'm not a lamb eater -RRB- I can say that I do n't even like Chicken Masala but I do like theirs': 1 Negative\n"
     ]
    }
   ],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "#in terminal:\n",
    "#cd stanford-corenlp-full-2018-10-05\n",
    "#java -mx5g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -timeout 10000\n",
    "\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "res = nlp.annotate(list(df.sentence)[1111],\n",
    "                   properties={'annotators': 'sentiment',\n",
    "                               'outputFormat': 'json',\n",
    "                               'timeout': 1000,\n",
    "                   })\n",
    "for s in res[\"sentences\"]:\n",
    "    print(\"%d: '%s': %s %s\" % (\n",
    "        s[\"index\"],\n",
    "        \" \".join([t[\"word\"] for t in s[\"tokens\"]]),\n",
    "        s[\"sentimentValue\"], s[\"sentiment\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(sentence):\n",
    "    try:\n",
    "        res = nlp.annotate(sentence,\n",
    "                       properties={'annotators': 'sentiment',\n",
    "                                   'outputFormat': 'json',\n",
    "                                   'timeout': 1000,\n",
    "                       })\n",
    "        result = (res['sentences'][0]['sentimentValue'], res['sentences'][0]['sentiment'])\n",
    "    except Exception as e:\n",
    "        result = ('error', e)\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['sentence'].isna()==False]\n",
    "df['sentiment'] = [get_sentiment(X) for X in df['sentence']]\n",
    "df['sentiment_val'] = [(X[0]) for X in df.sentiment]\n",
    "df['sentiment_val'] = [int(X) if X != 'error' else np.nan for X in df.sentiment_val]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(df,'pizza_sentences.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TOPIC MODELING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stopwords = ['way', 'honey', 'otto', 'pick', 'end', 'vega', 'libretto', 'hour',\n",
    "                 'place', 'vitos', 'wish', 'point', 'thats', 'bit', 'piece', 'word',\n",
    "                 'review', 'door', 'guy', 'ill', 'alla', 'im', 'yelp', 'type', 'montreal',\n",
    "                 'phone', 'aria', 'day', 'phoenix', 'star', 'ive', 'lot', 'need', 'table',\n",
    "                 'thing', 'mario', 'year', 'spot', 'rest']\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "for s in new_stopwords:\n",
    "    nlp.Defaults.stop_words.add(s)\n",
    "    lex = nlp.vocab[s]\n",
    "    lex.is_stop = True\n",
    "    nlp.vocab[s].is_stop = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_stopwords(tokenized_text):\n",
    "    tokenized_text = [t for t in tokenized_text if nlp.vocab[t].is_stop == False]\n",
    "    return(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics_lda(tokenized_text, num_topics):\n",
    "    id2word = gensim.corpora.Dictionary(tokenized_text)\n",
    "    corpus = [id2word.doc2bow(t) for t in tokenized_text]\n",
    "    lda_model = gensim.models.ldamulticore.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=128,\n",
    "                                           per_word_topics=True)\n",
    "    return(lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned'] = [clean_stopwords(t) for t in df['cleaned']]\n",
    "id2word = gensim.corpora.Dictionary(df.cleaned)\n",
    "corpus = [id2word.doc2bow(t) for t in df.cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_whole = get_topics_lda(df.cleaned, 5)\n",
    "lda_model_whole.print_topics(num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(lda_model_whole, 'lda_all.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_whole = pd.read_pickle('lda_all.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_document_topics = [lda_model_whole.get_document_topics(item) for item in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_document_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.081*\"order\" + 0.080*\"pizza\" + 0.039*\"cheese\" + 0.027*\"pepperoni\" + 0.026*\"minute\" + 0.019*\"slice\" + 0.018*\"sausage\" + 0.017*\"meal\" + 0.015*\"option\" + 0.015*\"lunch\" + 0.014*\"mushroom\" + 0.014*\"drink\" + 0.013*\"area\" + 0.013*\"pepper\" + 0.012*\"beer\" + 0.011*\"atmosphere\" + 0.011*\"way\" + 0.010*\"wine\" + 0.009*\"thing\" + 0.009*\"piece\"'),\n",
       " (1,\n",
       "  '0.106*\"food\" + 0.086*\"service\" + 0.038*\"delivery\" + 0.032*\"table\" + 0.022*\"review\" + 0.022*\"star\" + 0.019*\"guy\" + 0.018*\"hour\" + 0.014*\"phone\" + 0.013*\"bar\" + 0.011*\"year\" + 0.009*\"door\" + 0.009*\"bite\" + 0.008*\"room\" + 0.008*\"plate\" + 0.008*\"hotel\" + 0.007*\"party\" + 0.006*\"driver\" + 0.006*\"dining\" + 0.006*\"floor\"'),\n",
       " (2,\n",
       "  '0.052*\"sauce\" + 0.043*\"restaurant\" + 0.026*\"location\" + 0.025*\"cheese\" + 0.025*\"menu\" + 0.024*\"customer\" + 0.023*\"wing\" + 0.021*\"crust\" + 0.021*\"pie\" + 0.020*\"topping\" + 0.020*\"experience\" + 0.018*\"taste\" + 0.015*\"tomato\" + 0.014*\"flavor\" + 0.012*\"spot\" + 0.012*\"quality\" + 0.012*\"lot\" + 0.010*\"manager\" + 0.010*\"mozzarella\" + 0.010*\"kid\"'),\n",
       " (3,\n",
       "  '0.119*\"pizza\" + 0.111*\"place\" + 0.080*\"time\" + 0.032*\"crust\" + 0.030*\"slice\" + 0.024*\"staff\" + 0.024*\"love\" + 0.017*\"day\" + 0.015*\"style\" + 0.015*\"night\" + 0.013*\"server\" + 0.013*\"york\" + 0.012*\"bit\" + 0.010*\"line\" + 0.010*\"family\" + 0.008*\"couple\" + 0.008*\"min\" + 0.008*\"crispy\" + 0.007*\"dessert\" + 0.007*\"people\"'),\n",
       " (4,\n",
       "  '0.238*\"pizza\" + 0.033*\"salad\" + 0.025*\"price\" + 0.018*\"pasta\" + 0.016*\"bread\" + 0.013*\"chicken\" + 0.013*\"people\" + 0.010*\"dough\" + 0.010*\"dish\" + 0.010*\"meat\" + 0.009*\"business\" + 0.009*\"home\" + 0.009*\"friend\" + 0.009*\"oil\" + 0.008*\"choice\" + 0.008*\"oven\" + 0.008*\"vega\" + 0.007*\"sandwich\" + 0.007*\"dinner\" + 0.007*\"half\"')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model_whole.print_topics(num_words=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END NOTEBOOK**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
